{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't focus on complex model building from scratch, but rather on your ability to apply PyTorch for real-world tasks and problem-solving. Here are some questions you might encounter:\n",
    "\n",
    "### Q1. Data Loading and Preprocessing: \n",
    "Explain how to load a dataset using PyTorch's data loaders and write code to preprocess data (e.g., normalization, transformation) for a computer vision task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Define transformations (e.g., normalization)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data',                   \n",
    "                   # train parameter here decides whether the dataset is training \n",
    "                   # or testing as per the boolean flag                  \n",
    "                   train=True, \n",
    "                   download=True,\n",
    "                   # the downloaded data is in PIL format. We want to make it into Tensor.\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), \n",
    "                       transforms.Normalize((...),(...))\n",
    "                    ])),\n",
    "                    # shuffle set as True to make results generic\n",
    "                    batch_size=128, shuffle=True  \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Building a Simple CNN: \n",
    "Write PyTorch code to define a simple Convolutional Neural Network (CNN) architecture for image classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Define convolutional layers, pooling layers, and fully connected layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass through the network\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        x = self.conv1(x)\n",
    "        # ...\n",
    "        return F.log_softmax(x, ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Loss Function and Optimizer: \n",
    "Explain the purpose of loss functions and optimizers in neural network training. Write code to define a common loss function (e.g., Cross-Entropy) and an optimizer (e.g., Adam) for training your CNN.\n",
    "\n",
    "#### Loss Functions \n",
    "\n",
    "- **Measures Prediction Error:** It calculates the difference between the predicted output of the neural network and the actual target value. This difference represents the error or \"loss\" the network made in its prediction.\n",
    "- **Quantifies Model Performanc**&& By calculating the loss over a set of training data, the loss function provides a numerical score that reflects how well the model is performing overall. Lower loss indicates better alignment between predictions and actual values.\n",
    "- **Examples:** Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy for classification tasks.\n",
    "\n",
    "#### Optimizer:\n",
    "\n",
    "- **Adjusts Model Parameters:** Based on the calculated loss from the loss function, the optimizer iteratively tweaks the internal parameters (weights and biases) of the neural network.\n",
    "- **Minimizes Loss:** The optimizer aims to adjust these parameters in a way that minimizes the overall loss function. This essentially guides the network towards learning the patterns in the training data and making more accurate predictions.\n",
    "- **Algorithms:** Popular optimizers include Stochastic Gradient Descent (SGD) and its variants like Adam or RMSprop. These algorithms use the gradients (rate of change) of the loss function with respect to the parameters to determine the direction and magnitude of the adjustments.\n",
    "\n",
    "**Football Analogy**\n",
    "\n",
    "- Suppose you're a free-kick taker. Your target is to hit the goal while avoiding the goalkeeper, and the wall.\n",
    "- The distance between your hit ball and the goal post = Loss Function\n",
    "- The measures you take such as reducing stride length, changing foot, angle, force, point of contact = optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion1 = F.nll_loss(...)\n",
    "\n",
    "device = torch.device(...)  # 'mps', 'cuda' etc.\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Training Loop: \n",
    "Write Python code for a training loop that iterates over the dataset, performs forward pass, calculates loss, performs backward pass, and updates model weights using the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for data, target in train_loader:\n",
    "        # move data and target to the device e.g. graphics card. edge programming \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Forward pass and run data through the model   \n",
    "        output = model(data)\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target) # e.g. loss = nn.CrossEntropyLoss()\n",
    "        # update weights and set up fresh calculations\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass \n",
    "        loss.backward()\n",
    "        # changes the parameters in the network\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Evaluation Metrics: \n",
    "How would you evaluate the performance of your trained CNN model on a validation set? Write code to calculate accuracy or other relevant metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # ...\n",
    "        correct += (predictions == target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "accuracy = correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Debugging and Error Handling:\n",
    "How would you approach debugging errors that might arise during training (e.g., dimension mismatch)?\n",
    "\n",
    "- Use print statements or a debugger to inspect data shapes and identify mismatches.\n",
    "- Check for errors in device placement (CPU/GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Deployment Considerations: \n",
    "Briefly explain how you might consider deploying a PyTorch model for real-world inference on the edge (considering Axelera's focus on edge AI).\n",
    "\n",
    "- Explore libraries like ONNX for model conversion for efficient edge inference.\n",
    "- Consider resource constraints and optimize model size/complexity for edge deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. Choosing the Right Architecture: \n",
    "Given a specific computer vision task (e.g., object detection), how would you decide on the most suitable neural network architecture (e.g., CNN vs. Transformer)?\n",
    "\n",
    "- For object detection, consider architectures like YOLO, SSD, or Faster R-CNN, which excel at bounding box detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. Common Neural Network Challenges: \n",
    "Discuss some common challenges faced during neural network training (e.g., overfitting, vanishing gradients). How would you approach mitigating these issues?\n",
    "\n",
    "- **Overfitting**: Use techniques like dropout, data augmentation, or L1/L2 regularization.\n",
    "- **Vanishing gradients**: Use activation functions like ReLU or Leaky ReLU, or consider gradient clipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. Case Studies and Troubleshooting: \n",
    "Imagine a scenario where a client is experiencing poor performance with their Axelera.ai Metis solution. How would you approach troubleshooting the issue? What factors might you consider related to the neural network or the platform itself?\n",
    "\n",
    "- Gather information on the task, dataset, and training process.\n",
    "- Check for common issues like data imbalance, poor hyperparameter tuning, or hardware - limitations on the Metis platform.\n",
    "- Analyze network performance metrics and visualize outputs to identify potential bottlenecks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
