{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't focus on complex model building from scratch, but rather on your ability to apply PyTorch for real-world tasks and problem-solving. Here are some questions you might encounter:\n",
    "\n",
    "### Q1. Data Loading and Preprocessing: \n",
    "Explain how to load a dataset using PyTorch's data loaders and write code to preprocess data (e.g., normalization, transformation) for a computer vision task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Define transformations (e.g., normalization)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data',                   \n",
    "                   # train parameter here decides whether the dataset is training \n",
    "                   # or testing as per the boolean flag                  \n",
    "                   train=True, \n",
    "                   download=True,\n",
    "                   # the downloaded data is in PIL format. We want to make it into Tensor.\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), \n",
    "                       transforms.Normalize((...),(...))\n",
    "                    ])),\n",
    "                    # shuffle set as True to make results generic\n",
    "                    batch_size=128, shuffle=True  \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Building a Simple CNN: \n",
    "Write PyTorch code to define a simple Convolutional Neural Network (CNN) architecture for image classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Define convolutional layers, pooling layers, and fully connected layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass through the network\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        x = self.conv1(x)\n",
    "        # ...\n",
    "        return F.log_softmax(x, ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Loss Function and Optimizer: \n",
    "Explain the purpose of loss functions and optimizers in neural network training. Write code to define a common loss function (e.g., Cross-Entropy) and an optimizer (e.g., Adam) for training your CNN.\n",
    "\n",
    "#### Loss Functions \n",
    "\n",
    "- **Measures Prediction Error:** It calculates the difference between the predicted output of the neural network and the actual target value. This difference represents the error or \"loss\" the network made in its prediction.\n",
    "- **Quantifies Model Performanc**&& By calculating the loss over a set of training data, the loss function provides a numerical score that reflects how well the model is performing overall. Lower loss indicates better alignment between predictions and actual values.\n",
    "- **Examples:** Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy for classification tasks.\n",
    "\n",
    "#### Optimizer:\n",
    "\n",
    "- **Adjusts Model Parameters:** Based on the calculated loss from the loss function, the optimizer iteratively tweaks the internal parameters (weights and biases) of the neural network.\n",
    "- **Minimizes Loss:** The optimizer aims to adjust these parameters in a way that minimizes the overall loss function. This essentially guides the network towards learning the patterns in the training data and making more accurate predictions.\n",
    "- **Algorithms:** Popular optimizers include Stochastic Gradient Descent (SGD) and its variants like Adam or RMSprop. These algorithms use the gradients (rate of change) of the loss function with respect to the parameters to determine the direction and magnitude of the adjustments.\n",
    "\n",
    "**Football Analogy**\n",
    "\n",
    "- Suppose you're a free-kick taker. Your target is to hit the goal while avoiding the goalkeeper, and the wall.\n",
    "- The distance between your hit ball and the goal post = Loss Function\n",
    "- The measures you take such as reducing stride length, changing foot, angle, force, point of contact = optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion1 = F.nll_loss(...)\n",
    "\n",
    "device = torch.device(...)  # 'mps', 'cuda' etc.\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Training Loop: \n",
    "Write Python code for a training loop that iterates over the dataset, performs forward pass, calculates loss, performs backward pass, and updates model weights using the optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for data, target in train_loader:\n",
    "        # move data and target to the device e.g. graphics card. edge programming \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Forward pass and run data through the model   \n",
    "        output = model(data)\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target) # e.g. loss = nn.CrossEntropyLoss()\n",
    "        # update weights and set up fresh calculations\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass \n",
    "        loss.backward()\n",
    "        # changes the parameters in the network\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Evaluation Metrics: \n",
    "How would you evaluate the performance of your trained CNN model on a validation set? Write code to calculate accuracy or other relevant metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        # ...\n",
    "        correct += (predictions == target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "accuracy = correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Debugging and Error Handling:\n",
    "How would you approach debugging errors that might arise during training (e.g., dimension mismatch)?\n",
    "\n",
    "`=>`\n",
    "- Use print statements or a debugger to inspect data shapes and identify mismatches.\n",
    "- Check for errors in device placement (CPU/GPU)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Deployment Considerations: \n",
    "Briefly explain how you might consider deploying a PyTorch model for real-world inference on the edge (considering Ax's focus on edge AI).\n",
    "\n",
    "`=>`\n",
    "\n",
    "1. Model Optimization:\n",
    "- Serialize the trained PyTorch model using torch.save() to save the model's parameters, architecture, and other necessary information\n",
    "- Optimize the model size by pruning unnecessary layers, reducing parameters, or using quantization techniques to reduce precision\n",
    "\n",
    "2. Convert the PyTorch model to TorchScript, which allows the model to be executed without the need for a Python runtime\n",
    "- Hardware Acceleration:\n",
    "- Leverage GPU acceleration, if available in the deployment environment, to speed up inference\n",
    "- Explore the use of specialized hardware like ARM-based CPUs, GPUs, and NPUs for efficient real-time inference on edge devices\n",
    "\n",
    "3. Deployment Strategies:\n",
    "- Use a PyTorch-specific deployment framework like TorchServe or ExecuTorch to manage and serve the model\n",
    "- Integrate the PyTorch model into a C++ library that can run natively on mobile and embedded devices\n",
    "- Deploy the model directly on the edge device, bringing it closer to the data source or application\n",
    "\n",
    "4. Error Handling and Monitoring:\n",
    "- Implement appropriate error handling mechanisms to handle exceptions and edge cases gracefully\n",
    "- Monitor and log the performance of the deployed PyTorch model, including metrics like prediction accuracy, inference time, and resource utilization\n",
    "\n",
    "5. Integration with ML Pipelines:\n",
    "- Integrate the PyTorch model into a larger machine learning pipeline using frameworks like scikit-learn or TensorFlow\n",
    "- Ensure consistent input data preprocessing between the training and deployment phases\n",
    "\n",
    "6. Conversion and Interoperability:\n",
    "- Convert the PyTorch model to ONNX format and use ONNX Runtime for efficient deployment on edge devices\n",
    "- Explore the use of ONNX as a common format for interoperability between different machine learning frameworks\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. Choosing the Right Architecture: \n",
    "Given a specific computer vision task (e.g., object detection), how would you decide on the most suitable neural network architecture (e.g., CNN vs. Transformer)?\n",
    "\n",
    "`=>`\n",
    "\n",
    "- For object detection, consider architectures like YOLO, SSD, or Faster R-CNN, which excel at bounding box detection.\n",
    "\n",
    "1. **Convolutional Neural Networks (CNNs):**\n",
    "- Arguably the most widely used architecture for computer vision.\n",
    "CNNs are specifically designed to process grid-like data, such as images, by exploiting the inherent spatial relationships between pixels.\n",
    "Their architecture incorporates convolutional layers that learn filters to identify features at different levels of abstraction, from edges\n",
    " and lines to more complex shapes and objects.\n",
    "- Applications in computer vision: Image classification (recognizing objects in images), object detection (identifying and locating objects \n",
    "in images), image segmentation (classifying each pixel in an image to a specific category).\n",
    "\n",
    "2. **Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks:**\n",
    "- While not as prevalent as CNNs for general computer vision tasks, RNNs and LSTMs become crucial when dealing with sequential data like video.\n",
    "RNNs are capable of processing sequences by incorporating information from previous steps. LSTMs are a special type of RNN specifically \n",
    "designed to address the vanishing gradient problem, allowing them to capture long-term dependencies within sequences.\n",
    "- Applications in computer vision: Video analysis (action recognition in videos, anomaly detection in video surveillance), video captioning \n",
    "(generating descriptions of video content).\n",
    "\n",
    "3. **Transformers:**\n",
    "- This relatively new architecture has gained significant traction in computer vision tasks, particularly those involving object detection \n",
    "and image captioning.\n",
    "Transformers excel at capturing long-range dependencies within data using a self-attention mechanism. This allows them to analyze \n",
    "relationships between different parts of an image or sequence effectively.\n",
    "- Applications in computer vision: Object detection (achieving state-of-the-art performance in some cases), image captioning, visual question \n",
    "answering (answering questions based on an image).\n",
    "\n",
    "4. **Generative Adversarial Networks (GANs):**\n",
    "- While not strictly for classification or detection, GANs have interesting applications in computer vision tasks related to data generation.\n",
    "A GAN consists of two competing networks: a generator that learns to create new images, and a discriminator that tries to distinguish real \n",
    "images from generated ones.\n",
    "- Applications in computer vision: Generating realistic images (e.g., creating new product prototypes or editing existing ones), image \n",
    "inpainting (filling in missing parts of images).\n",
    "\n",
    "- Table : \n",
    "\n",
    "| Application | Model Type | Model Name | \n",
    "| ---| --- | --- |   \n",
    "| Object Detection | \tCNN | \tYOLO, EfficientDet, Mask R-CNN | \n",
    "| Face Recognition | \tCNN | \tEdgeFace | \n",
    "| Image Classification | \tCNN | \tMobileNet, ShuffleNet, VarGNet, MixNets | \n",
    "| Edge Computing | \tEdgeNeXt | \t- | \n",
    "| Real-time Video Processing | \tCNN | Shallow 3D(S3D), MobileNet  | \n",
    "| Autonomous Vehicles | CNN, Reinforcement Learning | \t- | \n",
    "| Smart Video Analytics | CNN | \t- | \n",
    "| Medical Imaging | CNN | \t- | \n",
    "| Industrial Inspection | CNN |\t- | \n",
    "| Augmented Reality | CNN |\t- | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. Common Neural Network Challenges: \n",
    "Discuss some common challenges faced during neural network training (e.g., overfitting, vanishing gradients). How would you approach mitigating these issues?\n",
    "\n",
    "`=>`\n",
    "- **Overfitting**: Use techniques like dropout, data augmentation, or L1/L2 regularization.\n",
    "- **Vanishing gradients**: Use activation functions like ReLU or Leaky ReLU, or consider gradient clipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10. Case Studies and Troubleshooting: \n",
    "Imagine a scenario where a client is experiencing poor performance with their AM solution. How would you approach troubleshooting the issue? What factors might you consider related to the neural network or the platform itself?\n",
    "\n",
    "`=>`\n",
    "- Gather information on the task, dataset, and training process.\n",
    "- Check for common issues like data imbalance, poor hyperparameter tuning, or hardware - limitations on the AM platform.\n",
    "- Analyze network performance metrics and visualize outputs to identify potential bottlenecks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
