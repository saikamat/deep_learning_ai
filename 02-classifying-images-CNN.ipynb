{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Images using CNN\n",
    "\n",
    "\n",
    "- Torch is main pytorch module. \n",
    "- The `nn` module contains things like layer definitions, activations, loss functions etc.\n",
    "- The helper module `functional` provides almost same functionality as the `nn` module. Don't need to initialize object for activation function.\n",
    "- The `optim` module contains the hyper-optimizers\n",
    "- `torchvision` is the computer vision module of pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F    \n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "- A simpler albeit less common way is the `Sequential()` way => define what layers your NN has.\n",
    "\n",
    "#### Pseudocode\n",
    "1. `import torch.nn as nn`\n",
    "2. Define the no. of units in each layer. e.g.\n",
    "   | Layer | No. of units / nodes / neurons |\n",
    "   |---|---|\n",
    "   | Input | 10 |\n",
    "   | Hidden | 5 |\n",
    "   | Output | 1 |\n",
    "\n",
    "3. Load the data\n",
    "4. Define the model\n",
    "\n",
    "   ```python\n",
    "   model = nn.Sequential(\n",
    "                  nn.Linear(10, 5),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(5, 1),\n",
    "                  nn.Sigmoid()\n",
    "            )\n",
    "   ```\n",
    "5. Define the `loss` function.\n",
    "   ```python\n",
    "   MSE = nn.MSELoss()\n",
    "   ```\n",
    "\n",
    "6. Optimization Algorithm\n",
    "   ```python\n",
    "   optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "   ```\n",
    "----\n",
    "\n",
    "- **However**, in Pytorch, to define the neural network model, you have to define class e.g. `Net`. It has to inherit from the `nn` module.\n",
    "- This is applicable to more complex NNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    - the __init__() constructor is called whenever we create instance of the `nn.Module`\n",
    "    - here we define no. of layers, neurons, hidden layers\n",
    "    - for CNNs, we alsod define kernel sizes, probability of dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Challenge with `__init__`:\n",
    "        \n",
    "        When creating a subclass, you might want to call the parent class's `__init__ `\n",
    "        function to initialize its attributes before adding your own customizations \n",
    "        in the subclass's `__init__`.\n",
    "        \n",
    "        Simply using ParentClass.`__init__(self)` within the subclass's `__init__` can \n",
    "        lead to issues, especially with multiple levels of inheritance (more than one \n",
    "        parent class).\n",
    "\n",
    "        Solution: `super()`\n",
    "        `super()` is a built-in function that helps you call the appropriate `__init__` \n",
    "        method from the parent class in a robust way.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        \"\"\"\n",
    "        - We have 4 layers: 2 convolutional and 2 fully connected (FC)\n",
    "        - First Conv layer accepts single 2D image and outputs 20 different matrices. The shape \n",
    "        of the output matrix depends on shape of kernel. That's 5x5 kernel in 3rd parameter\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        \n",
    "        \"\"\"\n",
    "        - Second Conv layer has same definition. 20 input features from output of previous layer.\n",
    "        Output 50 different features. And kernel size is 5x5. Output of the second conv. layer \n",
    "        when fed into 1st FC layer will be 50 channels and matrix size is 4x4.\n",
    "        \"\"\"\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        # self.fc3 = nn.Linear(1000, 10)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    - in `forward()`, we define how the output will be computed.\n",
    "    - In Pytorch, if you define forward pass through network, then it uses `autograd` \n",
    "    library to automatically calculate the backward pass.\n",
    "    - it takes `x` vector or tensor as input.\n",
    "    - the vector is of size 784 at the beginning.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # ReLU applied to output of first conv layer\n",
    "\n",
    "        \"\"\"\n",
    "        max pooling function to output of previous step i.e. x. It looks at matrices\n",
    "        of size 2x2 and move 2 pixels at a time.\n",
    "        \"\"\"\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "\n",
    "        # do exact the same for conv layer 2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        \"\"\"\n",
    "        # x.view(<batch-size>, 784). \n",
    "        # When batch-size = -1 => you don't care about the batch size.\n",
    "        # flatten the layers to one flat tensor of 800 neurons\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 4*4*50)   \n",
    "        \n",
    "        \"\"\"\n",
    "        # fc layer 1 is connected to input vector. \n",
    "        # And ReLU non linear activation function is applied to it.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))  \n",
    "        x = self.fc2(x)  # do the same but for layer two.\n",
    "        # x = self.fc3(x)          # don't apply AF here. Only Linear function\n",
    "\n",
    "        \"\"\"\n",
    "        - logarithmic softmax function applied here. Output of network isn't of size 10\n",
    "        where every value = probability\n",
    "        - softmax value which has highest probability.\n",
    "        \"\"\"\n",
    "        return F.log_softmax(x, dim=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "- We use `DataLoader` and `DataSet` `torch` `util` classes\n",
    "- The training and testing will stay pretty much the same as previous example, until you get to the more advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data',                   \n",
    "                   # train parameter here decides whether the dataset is training \n",
    "                   # or testing as per the boolean flag                  \n",
    "                   train=True, \n",
    "                   download=True,\n",
    "                   # the downloaded data is in PIL format. We want to make it into Tensor.\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), \n",
    "                       transforms.Normalize((0.1307,),(0.3081,))\n",
    "                    ])),\n",
    "                    # shuffle set as True to make results generic\n",
    "                    batch_size=128, shuffle=True  \n",
    "                )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', \n",
    "                   train=False, \n",
    "                   download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(), \n",
    "                       transforms.Normalize((0.1307,),(0.3081,))\n",
    "                    ])),\n",
    "                    batch_size=1000, shuffle=True\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing loops\n",
    "\n",
    "- Boiler plate code for training and testing.\n",
    "- Most ML Frameworks don't expose this part of code.\n",
    "- However since PyTorch framework is used for experimental research, you have full control over how a model is trained/tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the train() accepts the \n",
    "    neural network `model`, \n",
    "    the `device` on which to run it, \n",
    "    the `train_loader` training dataset, epoch and optimizer\n",
    "\"\"\"\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()   # biases in the model can be changed.\n",
    "\n",
    "    # running with enum, coz wanna know batch index current loss\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): \n",
    "        # move data and target to the device e.g. graphics card. edge programming \n",
    "        data, target = data.to(device), target.to(device)   \n",
    "        optimizer.zero_grad()  # set up fresh calculations\n",
    "        output = model(data)   # running data through fwd pass and get output at current step\n",
    "\n",
    "        # loss at curr. step using negative log likelihood loss coz it works well \n",
    "        # for softmax output\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        # PyTorch feature. By doing back-propagation through network, can be done automatically.\n",
    "        loss.backward()    \n",
    "        optimizer.step()   # changes the parameters in the network\n",
    "        if batch_idx%100 == 0:\n",
    "            # print every 100 batches\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx*len(data), len(train_loader.dataset), 100.*batch_idx/len(train_loader), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()   # model during eval mode will NOT change parameters\n",
    "    test_loss = 0\n",
    "    correct = 0    # we count how many test sets got correct\n",
    "    with torch.no_grad():  # speeds up the process. We tell don't remeber gradients so make faster processing\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)   # running the data through the model\n",
    "\n",
    "            # since we're testing in batch, we're interested in only sum of all losses in the batch\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "\n",
    "            # get index of maxiumum o/p probability from the o/p vector\n",
    "            pred = output.argmax(dim=1, keepdim=True)   \n",
    "\n",
    "            # count how many samples we got correct/incorrect. \n",
    "            # Compare the correct labels in batch and sum them up.\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest Set: Average Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), 100.*correct/len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.297693\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.172063\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.131213\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.026413\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.081763\n",
      "\n",
      "Test Set: Average Loss: 0.0615, Accuracy: 9794/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.062588\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.045184\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.088043\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.043429\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.050045\n",
      "\n",
      "Test Set: Average Loss: 0.0422, Accuracy: 9849/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.044380\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.081854\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.015940\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.048186\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.067345\n",
      "\n",
      "Test Set: Average Loss: 0.0329, Accuracy: 9894/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(42) # for reproducible results, set random weight\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"mps\") # PyTorch added support for M1 GPU on May 2022\n",
    "\n",
    "model = Net().to(device)  # Move the model to the device.\n",
    "\"\"\"\n",
    "- We've to tell it what it is optimizing. \n",
    "- A helper method parameters() can be called in any neural network model in PyTorch\n",
    "- It calls all the params based on the fwd function.\n",
    "\"\"\"\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.5) \n",
    "\n",
    "# test before training\n",
    "\n",
    "for epoch in range(1, 3+1):  # train for 3 epochs, and test after running each epoch.\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist.pt\")  # save the model to `mnist.pt`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x3521b67d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test_image = test_loader.dataset.data[8080]\n",
    "plt.imshow(test_image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1848.5481, -1359.6426, -1290.8562,     0.0000, -1820.7024, -1367.5131,\n",
       "         -2498.4868, -1442.9772, -1343.4097, -1162.1522]], device='mps:0',\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THE IMAGE THROUGH MODEL\n",
    "#out = model(test_image.view(1, 1, 28, 28).float().cuda()))  <--- for CUDA support\n",
    "\n",
    "\"\"\"\n",
    "we re-shape the input to the network to a tensor of 4 dimensions\n",
    "1 => no. of images in batch\n",
    "1 => no. of channels. Here it's grayscale, therefore 1. For RGB, it'll be 3.\n",
    "28, 28 => size of the image\n",
    "\"\"\"\n",
    "out = model(test_image.view(1, 1, 28, 28).float().to(torch.device('mps')))  # <--- for Mac M1 GPUs\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in above output, all values in the 10 unit vector output are negative or zero except for the index value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3], device='mps:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More PyTorch models\n",
    "\n",
    "- You may have noticed that we typically defined our models ourselves e.g. in `Net()` class above.\n",
    "- But PyTorch has many more widely known models as stated below, which are already trained aka pre-trained.\n",
    "- So we need not train them separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlexNet',\n",
       " 'AlexNet_Weights',\n",
       " 'ConvNeXt',\n",
       " 'ConvNeXt_Base_Weights',\n",
       " 'ConvNeXt_Large_Weights',\n",
       " 'ConvNeXt_Small_Weights',\n",
       " 'ConvNeXt_Tiny_Weights',\n",
       " 'DenseNet',\n",
       " 'DenseNet121_Weights',\n",
       " 'DenseNet161_Weights',\n",
       " 'DenseNet169_Weights',\n",
       " 'DenseNet201_Weights',\n",
       " 'EfficientNet',\n",
       " 'EfficientNet_B0_Weights',\n",
       " 'EfficientNet_B1_Weights',\n",
       " 'EfficientNet_B2_Weights',\n",
       " 'EfficientNet_B3_Weights',\n",
       " 'EfficientNet_B4_Weights',\n",
       " 'EfficientNet_B5_Weights',\n",
       " 'EfficientNet_B6_Weights',\n",
       " 'EfficientNet_B7_Weights',\n",
       " 'EfficientNet_V2_L_Weights',\n",
       " 'EfficientNet_V2_M_Weights',\n",
       " 'EfficientNet_V2_S_Weights',\n",
       " 'GoogLeNet',\n",
       " 'GoogLeNetOutputs',\n",
       " 'GoogLeNet_Weights',\n",
       " 'Inception3',\n",
       " 'InceptionOutputs',\n",
       " 'Inception_V3_Weights',\n",
       " 'MNASNet',\n",
       " 'MNASNet0_5_Weights',\n",
       " 'MNASNet0_75_Weights',\n",
       " 'MNASNet1_0_Weights',\n",
       " 'MNASNet1_3_Weights',\n",
       " 'MaxVit',\n",
       " 'MaxVit_T_Weights',\n",
       " 'MobileNetV2',\n",
       " 'MobileNetV3',\n",
       " 'MobileNet_V2_Weights',\n",
       " 'MobileNet_V3_Large_Weights',\n",
       " 'MobileNet_V3_Small_Weights',\n",
       " 'RegNet',\n",
       " 'RegNet_X_16GF_Weights',\n",
       " 'RegNet_X_1_6GF_Weights',\n",
       " 'RegNet_X_32GF_Weights',\n",
       " 'RegNet_X_3_2GF_Weights',\n",
       " 'RegNet_X_400MF_Weights',\n",
       " 'RegNet_X_800MF_Weights',\n",
       " 'RegNet_X_8GF_Weights',\n",
       " 'RegNet_Y_128GF_Weights',\n",
       " 'RegNet_Y_16GF_Weights',\n",
       " 'RegNet_Y_1_6GF_Weights',\n",
       " 'RegNet_Y_32GF_Weights',\n",
       " 'RegNet_Y_3_2GF_Weights',\n",
       " 'RegNet_Y_400MF_Weights',\n",
       " 'RegNet_Y_800MF_Weights',\n",
       " 'RegNet_Y_8GF_Weights',\n",
       " 'ResNeXt101_32X8D_Weights',\n",
       " 'ResNeXt101_64X4D_Weights',\n",
       " 'ResNeXt50_32X4D_Weights',\n",
       " 'ResNet',\n",
       " 'ResNet101_Weights',\n",
       " 'ResNet152_Weights',\n",
       " 'ResNet18_Weights',\n",
       " 'ResNet34_Weights',\n",
       " 'ResNet50_Weights',\n",
       " 'ShuffleNetV2',\n",
       " 'ShuffleNet_V2_X0_5_Weights',\n",
       " 'ShuffleNet_V2_X1_0_Weights',\n",
       " 'ShuffleNet_V2_X1_5_Weights',\n",
       " 'ShuffleNet_V2_X2_0_Weights',\n",
       " 'SqueezeNet',\n",
       " 'SqueezeNet1_0_Weights',\n",
       " 'SqueezeNet1_1_Weights',\n",
       " 'SwinTransformer',\n",
       " 'Swin_B_Weights',\n",
       " 'Swin_S_Weights',\n",
       " 'Swin_T_Weights',\n",
       " 'Swin_V2_B_Weights',\n",
       " 'Swin_V2_S_Weights',\n",
       " 'Swin_V2_T_Weights',\n",
       " 'VGG',\n",
       " 'VGG11_BN_Weights',\n",
       " 'VGG11_Weights',\n",
       " 'VGG13_BN_Weights',\n",
       " 'VGG13_Weights',\n",
       " 'VGG16_BN_Weights',\n",
       " 'VGG16_Weights',\n",
       " 'VGG19_BN_Weights',\n",
       " 'VGG19_Weights',\n",
       " 'ViT_B_16_Weights',\n",
       " 'ViT_B_32_Weights',\n",
       " 'ViT_H_14_Weights',\n",
       " 'ViT_L_16_Weights',\n",
       " 'ViT_L_32_Weights',\n",
       " 'VisionTransformer',\n",
       " 'Weights',\n",
       " 'WeightsEnum',\n",
       " 'Wide_ResNet101_2_Weights',\n",
       " 'Wide_ResNet50_2_Weights',\n",
       " '_GoogLeNetOutputs',\n",
       " '_InceptionOutputs',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_api',\n",
       " '_meta',\n",
       " '_utils',\n",
       " 'alexnet',\n",
       " 'convnext',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'detection',\n",
       " 'efficientnet',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'get_model',\n",
       " 'get_model_builder',\n",
       " 'get_model_weights',\n",
       " 'get_weight',\n",
       " 'googlenet',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'list_models',\n",
       " 'maxvit',\n",
       " 'maxvit_t',\n",
       " 'mnasnet',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mobilenetv2',\n",
       " 'mobilenetv3',\n",
       " 'optical_flow',\n",
       " 'quantization',\n",
       " 'regnet',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'segmentation',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'shufflenetv2',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_transformer',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'video',\n",
       " 'vision_transformer',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models\n",
    "dir(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
